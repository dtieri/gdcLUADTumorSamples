{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import corex as ce\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corex, rep size: 20 3\n",
      "Marginal description:  gaussian\n",
      "[ 0.254  0.03   0.129  1.146  0.193  0.087  0.16   0.104  0.255  0.085  0.06   0.055  0.091  0.166  0.019  0.284  0.023  0.208  0.157  0.216]\n",
      "[  8.609  -0.012   1.788  20.669   0.241   2.755  -0.029   0.809   6.019   0.02   -0.007   0.734   5.641  10.331   0.377   1.372   0.05    2.431   0.55    0.067]\n",
      "[  8.115   0.      4.059   8.73    2.754  22.529   0.287   0.362   2.578   0.412   2.199   7.241   6.078   1.473   5.307   0.873   6.612   3.396   1.073   0.257]\n",
      "[  3.858   0.02    3.612  11.106   4.195  25.36    1.347   3.456   5.472   2.877   3.829   3.051   3.837   2.806   4.978   2.494  10.637   6.747   1.952   0.506]\n",
      "[  2.734   0.889   3.684  10.341   3.77   28.416   1.688   5.026   6.678   1.891   4.265   2.29    3.897   3.216   6.609   3.073  11.03    7.581   2.442   0.914]\n",
      "[  1.217   2.305   3.375   9.273   3.312  29.036   1.616   6.367   8.003   1.546   5.448   2.084   3.811   4.977   7.17    3.776  11.02    7.226   2.738   1.246]\n",
      "[  1.126   4.891   2.898   8.565   3.027  29.734   0.985   7.344   8.667   1.365   5.535   1.817   3.089   5.562   7.328   4.259   9.493   7.492   2.609   2.325]\n",
      "[  1.242   5.264   2.871   8.262   2.683  30.232   0.865   8.023   9.362   1.294   5.539   1.826   2.885   6.432   7.896   5.173   7.677   7.638   2.2     2.713]\n",
      "[  1.597   5.383   2.904   7.986   2.754  29.773   0.801   8.201   9.496   1.213   5.728   1.683   2.901   6.949   8.048   5.755   7.012   7.457   2.535   3.613]\n",
      "[  1.656   5.369   2.942   7.313   2.249  29.102   0.755   9.029   9.17    1.275   5.641   1.521   3.659   7.115   8.215   6.268   6.38    7.107   2.653   4.486]\n",
      "[  1.541   5.283   2.967   7.05    1.94   29.022   0.838   7.884   9.386   1.292   5.612   1.562   3.967   8.001   8.437   6.28    5.907   7.061   2.961   5.418]\n",
      "[  1.773   5.196   2.957   6.951   1.731  28.558   0.774   7.795   9.479   1.368   5.386   1.583   4.18    8.143   8.298   6.415   5.894   6.576   3.299   5.913]\n",
      "[  1.594   4.865   2.965   6.822   1.621  28.412   0.761   8.181   9.408   1.593   5.457   1.753   4.39    7.798   8.096   6.428   5.863   6.596   3.298   6.252]\n",
      "[  1.839   4.755   2.948   7.253   1.499  28.017   0.65    8.358   9.242   1.651   5.432   2.118   4.06    7.405   8.172   6.583   5.705   6.716   3.142   6.574]\n",
      "[  1.757   4.725   2.977   7.437   1.422  27.355   0.629   8.735   9.421   1.786   5.059   2.265   3.942   7.62    8.053   7.19    5.644   6.849   3.12    6.431]\n",
      "[  2.255   4.328   2.966   7.689   1.461  27.388   0.63    8.19    9.436   1.876   5.066   2.375   3.668   7.431   7.92    7.534   5.438   7.11    3.15    6.501]\n",
      "[  2.331   4.022   2.969   7.086   1.507  28.03    0.436   7.992   9.43    1.919   5.093   2.96    3.751   7.975   7.84    7.5     4.657   7.091   3.457   6.519]\n",
      "[  2.566   3.641   2.943   7.384   1.543  27.084   0.4     7.98    9.449   1.896   5.072   3.442   3.596   8.08    7.801   8.092   4.901   6.919   3.553   6.388]\n",
      "[  2.557   3.426   2.915   7.584   1.648  26.603   0.33    8.432  10.003   2.029   4.939   3.922   3.337   7.653   7.387   7.919   4.728   7.152   3.814   6.37 ]\n",
      "[  2.401   2.691   2.71    7.768   1.553  25.924   0.252   8.109   9.907   2.18    4.81    4.752   3.136   8.271   7.434   8.365   5.142   6.642   3.83    6.202]\n",
      "[  2.301   2.806   2.729   7.859   1.555  24.503   0.228   8.219  10.012   2.247   4.715   5.052   2.634   8.564   7.01    9.68    5.108   6.99    3.791   6.271]\n",
      "[  2.174   3.224   2.728   7.539   1.327  23.614   0.21    8.087   9.689   2.015   4.551   4.947   2.515   8.024   7.052  11.513   4.906   6.371   3.758   6.502]\n",
      "[  1.991   3.427   2.699   7.829   1.299  22.419   0.203   8.394  10.031   2.173   4.346   5.156   2.327   7.704   7.023  11.886   4.96    6.194   3.757   6.864]\n",
      "[  1.898   3.345   2.718   7.931   1.308  21.614   0.202   8.105  10.054   2.247   4.283   5.261   2.383   7.831   7.072  12.968   4.794   6.523   3.627   6.995]\n",
      "[  1.781   3.403   2.732   7.856   1.291  22.543   0.192   7.691   9.899   2.333   4.294   5.181   2.18    7.691   6.919  13.06    4.559   6.688   3.719   7.114]\n",
      "[  1.808   3.136   2.679   7.418   1.292  22.406   0.198   7.557   9.856   2.284   4.223   5.195   2.007   7.91    7.044  13.633   4.395   6.785   3.806   7.689]\n",
      "[  1.813   2.9     2.714   7.486   1.33   22.296   0.185   7.549   9.891   2.372   4.249   5.036   2.122   8.393   7.143  13.619   4.377   6.759   3.81    7.678]\n",
      "[  1.645   3.055   2.714   7.772   1.402  22.797   0.192   7.308   9.6     2.355   4.303   5.095   2.025   8.261   7.21   13.66    4.973   6.165   3.75    7.546]\n",
      "[  1.598   3.358   2.66    7.594   1.506  23.399   0.187   7.066   9.38    2.591   4.571   4.882   1.984   8.408   7.448  13.047   4.984   5.927   3.994   7.547]\n",
      "[  1.47    3.083   2.689   7.777   1.46   22.968   0.202   6.927   9.365   3.129   4.484   4.582   2.194   8.351   7.416  13.77    5.248   5.543   3.752   7.472]\n",
      "[  1.388   3.028   2.656   7.611   1.461  22.847   0.22    6.726   9.284   4.554   4.454   4.535   2.265   8.08    7.077  13.523   5.204   5.44    3.702   7.789]\n",
      "[  1.362   3.194   2.619   7.607   1.548  22.569   0.251   6.515   9.259   6.065   4.339   4.514   2.224   7.458   7.051  13.499   4.812   4.939   3.732   7.69 ]\n",
      "[  1.36    3.256   2.634   6.829   1.466  22.019   0.261   6.002   9.297   9.425   4.506   4.368   2.257   7.149   6.926  13.388   3.871   4.644   3.814   7.295]\n",
      "[  1.296   3.38    2.647   6.794   1.446  21.981   0.27    6.425   8.953   8.112   4.44    4.309   2.299   7.22    7.01   14.242   4.828   4.575   4.051   7.032]\n",
      "[  1.327   3.13    2.667   7.258   1.491  22.124   0.27    6.214   9.156   7.85    4.462   4.267   2.234   6.898   6.986  14.406   4.779   4.514   4.025   7.148]\n",
      "[  1.321   3.171   2.692   7.183   1.507  22.494   0.274   5.688   9.329   7.499   4.429   4.084   2.553   6.855   7.087  15.089   4.767   4.622   3.732   6.91 ]\n",
      "[  1.337   3.359   2.712   7.159   1.449  22.796   0.279   6.05    8.927   6.983   4.487   3.811   2.399   7.223   7.098  14.87    5.57    4.236   4.016   6.849]\n",
      "[  1.245   3.637   2.668   7.109   1.422  23.365   0.278   6.076   9.288   7.294   4.306   3.684   2.378   7.605   6.744  14.126   5.424   4.415   4.086   6.809]\n",
      "[  1.251   3.451   2.704   7.467   1.431  23.232   0.286   6.166   9.129   6.984   4.315   3.57    2.552   7.531   6.693  14.092   5.702   4.501   4.24    7.053]\n",
      "[  1.301   3.584   2.668   7.49    1.483  23.215   0.274   6.297   9.216   7.118   4.266   3.106   2.916   7.198   6.641  13.736   6.009   4.407   4.442   6.957]\n",
      "[  1.335   3.754   2.693   7.446   1.432  22.795   0.29    6.365   9.234   7.356   4.275   3.007   2.937   7.278   6.882  13.636   5.819   4.296   4.949   7.131]\n",
      "[  1.368   3.793   2.683   7.518   1.444  22.397   0.282   6.202   9.395   6.98    4.243   3.225   2.928   7.478   6.68   13.447   6.06    4.129   5.726   7.504]\n",
      "[  1.397   3.739   2.698   7.349   1.459  22.579   0.284   6.215   9.509   6.87    4.238   3.433   2.98    7.322   7.124  13.687   5.491   4.178   5.858   7.334]\n",
      "[  1.469   3.754   2.744   7.22    1.471  22.18    0.302   5.992   9.393   7.009   4.266   3.441   2.969   7.267   6.866  13.657   5.836   4.378   6.349   7.146]\n",
      "[  1.491   3.492   2.693   7.48    1.457  22.479   0.303   5.924   9.305   6.57    4.235   3.459   2.961   7.216   7.142  13.677   5.492   3.899   6.772   6.905]\n",
      "[  1.569   3.271   2.701   7.585   1.469  22.271   0.308   5.539   9.332   6.425   4.305   3.44    2.928   7.252   7.105  13.72    5.389   3.85    7.113   6.811]\n",
      "[  1.653   3.569   2.707   7.499   1.525  22.565   0.297   5.56    9.263   6.122   4.344   3.407   3.024   7.175   7.083  13.16    5.158   3.91    7.311   6.941]\n",
      "[  1.732   3.626   2.705   7.558   1.42   21.96    0.286   5.584   9.241   5.628   4.344   3.418   2.956   7.28    7.059  13.533   5.129   4.205   7.645   6.892]\n",
      "[  1.903   3.336   2.709   7.284   1.435  21.848   0.283   5.753   9.166   5.733   4.623   3.355   2.822   7.305   6.951  13.198   5.238   3.962   7.928   7.162]\n",
      "[  2.032   3.282   2.712   7.173   1.462  22.011   0.307   5.645   9.228   6.131   4.654   3.374   2.82    7.166   6.89   12.321   5.416   3.901   8.427   7.096]\n",
      "[  1.983   3.19    2.646   6.81    1.467  20.321   0.283   5.556   8.782   6.676   4.615   3.358   2.919   7.115   6.585  11.675   5.761   3.937  10.585   7.027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.926   2.972   2.672   6.303   1.298  16.801   0.288   5.16    8.521   6.752   4.407   3.385   2.869   7.323   6.578  10.136   5.972   3.708  15.68    7.29 ]\n",
      "[  1.953   2.83    2.643   5.644   1.278  12.338   0.096   5.036   8.207   6.221   4.362   3.34    2.776   6.907   6.563   8.462   6.026   3.564  21.256   8.657]\n",
      "[  1.863   2.434   2.68    5.449   1.035  10.452   0.122   4.775   7.8     6.567   4.273   3.209   2.857   6.74    6.319   7.672   5.532   3.467  24.192   9.135]\n",
      "[  1.96    2.477   2.656   5.442   0.922  11.417   0.124   4.823   7.81    6.157   4.148   3.268   2.977   7.019   6.309   7.46    5.688   3.441  24.196   8.884]\n",
      "[  1.817   2.577   2.638   5.33    0.853  11.468   0.147   5.289   7.748   6.334   4.216   3.38    2.949   7.342   6.457   6.587   5.937   3.713  24.132   8.8  ]\n",
      "[  1.698   2.48    2.637   5.19    0.84   11.556   0.149   5.276   7.76    6.524   4.227   3.346   3.271   7.326   6.436   6.172   5.754   3.648  24.436   8.945]\n",
      "[  1.721   2.574   2.668   4.988   0.834  11.393   0.163   5.62    7.775   6.74    4.24    3.337   3.44    7.216   6.503   6.101   6.039   3.642  24.439   8.986]\n",
      "[  1.661   2.443   2.704   4.752   0.828  11.701   0.168   5.354   7.623   7.038   4.268   3.316   3.416   6.97    6.493   5.958   6.299   3.726  24.352   9.328]\n",
      "[  1.65    2.387   2.664   4.739   0.828  11.451   0.145   5.249   7.76    7.868   4.273   3.358   3.238   6.63    6.797   5.568   6.44    3.664  24.248   9.424]\n",
      "[  1.586   2.191   2.757   4.633   0.838  10.909   0.149   5.501   7.63    7.687   4.218   3.345   3.296   6.968   6.89    5.669   6.13    3.66   25.059   9.285]\n",
      "[  1.567   2.376   2.813   4.575   0.847  11.509   0.146   5.294   7.717   7.618   4.246   3.351   3.367   6.914   6.683   5.44    6.124   3.845  24.422   9.35 ]\n",
      "[  1.533   2.403   2.766   4.691   0.844  11.611   0.139   5.369   7.727   7.28    4.252   3.252   2.971   7.086   6.587   5.292   6.383   3.907  24.767   9.395]\n",
      "[  1.43    2.341   2.762   4.511   0.858  11.26    0.149   5.688   7.777   7.233   4.23    3.359   3.125   7.255   6.603   5.153   6.607   3.831  24.787   9.415]\n",
      "[  1.499   2.288   2.687   4.472   0.819  11.277   0.144   5.547   7.833   7.277   4.26    3.365   3.38    7.225   6.679   4.969   6.529   3.795  24.843   9.438]\n",
      "[  1.409   2.278   2.732   4.399   0.805  11.494   0.148   5.701   7.847   7.387   4.223   3.363   3.151   7.385   6.832   5.336   6.034   4.069  24.208   9.483]\n",
      "[  1.434   2.35    2.706   4.685   0.809  11.387   0.147   5.762   7.921   7.155   4.206   3.354   3.271   7.288   6.906   4.733   5.923   3.826  24.716   9.56 ]\n",
      "[  1.437   2.252   2.649   4.802   0.834  11.035   0.153   5.789   7.711   6.878   4.237   3.357   3.501   7.285   6.753   5.051   5.998   4.089  24.845   9.505]\n",
      "[  1.475   2.139   2.679   4.795   0.804  11.825   0.152   5.753   7.828   7.458   4.039   3.356   3.521   7.307   6.739   4.931   5.781   4.265  24.025   9.542]\n",
      "[  1.467   2.139   2.676   4.748   0.847  12.048   0.163   5.756   7.681   7.488   4.231   3.34    3.293   7.384   6.899   4.902   5.551   4.078  23.824   9.786]\n",
      "[  1.517   2.127   2.682   4.669   0.858  11.776   0.175   5.789   7.575   7.61    4.164   3.402   3.505   7.537   6.648   5.052   5.9     4.185  23.914   9.535]\n",
      "[  1.454   1.858   2.706   4.576   0.806  12.062   0.161   5.918   7.661   7.402   4.154   3.366   3.35    7.544   6.815   5.05    5.547   4.317  23.955   9.688]\n",
      "[  1.446   1.965   2.692   4.575   0.802  12.223   0.183   5.836   7.626   7.32    3.928   3.365   3.509   7.471   6.86    5.      5.626   4.122  24.029   9.667]\n",
      "[  1.552   1.9     2.718   4.32    0.81   11.539   0.157   5.828   7.676   7.697   4.158   3.394   3.572   7.256   6.795   4.954   5.714   4.066  24.242  10.153]\n",
      "[  1.478   1.922   2.699   4.189   0.836  12.332   0.179   6.005   7.93    7.417   4.17    3.385   3.298   7.14    7.04    4.806   5.624   4.233  23.59   10.283]\n",
      "[  1.463   1.922   2.715   4.308   0.809  12.395   0.18    5.937   7.919   7.537   4.126   3.344   3.586   6.85    6.94    4.861   5.408   4.145  23.894  10.031]\n",
      "[  1.54    1.881   2.695   4.204   0.8    12.259   0.184   5.989   7.913   7.516   4.146   3.334   3.37    6.955   6.932   4.811   5.479   4.237  23.974  10.119]\n",
      "[  1.434   1.918   2.819   4.378   0.804  12.421   0.17    6.145   7.91    7.371   4.139   3.343   3.536   6.823   6.864   4.604   5.713   3.913  24.035  10.195]\n",
      "[  1.473   1.897   2.749   4.333   0.788  12.227   0.177   6.199   7.997   7.398   4.134   3.353   3.564   6.875   7.003   4.747   5.523   3.825  24.058  10.184]\n",
      "[  1.508   1.927   2.963   4.392   0.795  12.454   0.177   6.201   7.814   7.172   4.119   3.341   3.456   6.696   6.722   4.662   5.647   3.761  24.187  10.508]\n",
      "[  1.473   1.752   2.992   4.172   0.844  12.636   0.167   6.11    7.771   6.922   4.116   3.345   3.682   6.626   6.607   4.642   6.16    3.443  24.436  10.325]\n",
      "[  1.467   1.714   2.944   4.388   0.842  14.134   0.165   6.155   7.809   6.757   4.1     3.331   3.774   6.697   6.583   4.593   5.47    3.503  23.679  10.045]\n",
      "[  1.385   1.845   2.932   4.445   0.964  15.138   0.163   6.077   7.958   6.448   4.138   3.329   3.918   6.639   6.443   4.736   5.404   3.386  22.738  10.236]\n",
      "[  1.353   1.744   2.984   4.464   1.021  15.101   0.166   6.097   8.103   6.223   4.147   3.347   3.792   6.418   6.419   5.08    5.192   3.232  23.536  10.316]\n",
      "[  1.414   1.72    2.957   4.398   1.042  16.602   0.169   6.041   8.188   6.023   4.14    3.342   3.512   6.749   6.208   4.927   5.147   3.389  22.133  10.676]\n",
      "[  1.621   1.68    2.914   4.293   1.062  18.06    0.173   6.128   8.521   6.054   4.139   3.327   3.473   6.406   6.369   4.611   5.298   3.369  21.046  10.904]\n",
      "[  1.585   1.831   2.925   4.17    1.12   20.999   0.172   6.272   8.483   6.711   4.17    3.32    3.639   6.465   6.466   4.853   5.216   3.537  17.657  10.928]\n",
      "[  1.602   2.02    2.697   4.208   1.199  22.012   0.175   6.376   8.582   7.009   3.932   3.331   3.57    6.303   6.338   5.034   5.273   3.232  16.659  11.035]\n",
      "[  1.577   2.033   2.767   4.221   1.111  22.937   0.159   6.669   8.48    6.526   3.951   3.318   3.538   6.529   6.578   5.172   5.466   3.341  15.329  11.601]\n",
      "[  1.569   2.23    2.778   4.321   1.107  24.212   0.17    7.486   8.329   6.902   3.951   3.198   3.623   6.523   6.546   5.554   5.188   3.78   13.031  11.163]\n",
      "[  1.612   2.066   2.777   4.471   1.037  24.569   0.176   7.399   8.629   7.141   3.961   3.163   3.551   6.817   6.487   5.472   5.204   4.211  11.759  11.364]\n",
      "[  1.507   2.032   2.706   4.687   1.006  25.394   0.159   7.492   8.658   7.147   3.63    3.156   3.596   6.945   6.603   5.886   5.516   4.299  10.093  11.42 ]\n",
      "[  1.528   2.015   2.753   4.728   0.997  25.935   0.158   7.712   8.689   7.172   3.492   3.149   3.947   6.821   6.442   5.674   5.648   4.272   9.311  11.439]\n",
      "[  1.535   2.019   2.808   4.657   0.989  25.954   0.154   7.662   8.53    7.337   3.461   3.147   4.214   6.882   6.527   5.701   5.807   4.061   8.593  11.806]\n",
      "[  1.628   2.026   2.693   4.762   1.015  26.32    0.154   7.883   8.609   7.13    3.481   3.174   4.126   6.845   6.464   5.578   5.854   3.942   8.226  11.924]\n",
      "[  1.658   1.957   2.72    4.63    0.996  26.401   0.149   7.776   8.345   7.289   3.476   3.2     4.221   6.604   6.681   5.796   6.119   3.972   7.902  11.967]\n",
      "[  1.522   2.001   2.712   4.533   0.969  26.732   0.16    7.805   7.59    7.074   3.505   3.155   4.16    6.865   6.561   5.607   6.003   3.936   7.67   12.509]\n",
      "[  1.581   1.948   2.693   4.359   0.929  27.31    0.157   7.435   7.724   7.635   3.48    3.183   4.208   6.655   6.48    5.416   6.443   3.889   6.961  12.406]\n",
      "[  1.574   2.031   2.747   4.433   0.875  27.527   0.155   7.442   7.637   7.078   3.46    3.153   4.168   6.705   6.427   5.456   6.368   3.736   6.989  12.478]\n",
      "[  1.586   2.007   2.664   4.286   0.865  27.609   0.15    7.651   7.834   7.581   3.471   3.195   4.006   6.511   6.427   5.323   6.653   3.565   6.46   12.35 ]\n",
      "Overall tc: 120.194053205\n",
      "Best tc: 120.194053205\n"
     ]
    }
   ],
   "source": [
    "X=pd.read_csv('LUADGeneTableDE.csv',index_col=0)\n",
    "\n",
    "layer1 = ce.Corex(n_hidden=20,dim_hidden=3, marginal_description='gaussian', smooth_marginals=True, verbose=1,ram=4)  \n",
    "layer2 = ce.Corex(n_hidden=10)\n",
    "layer3 = ce.Corex(n_hidden=5)\n",
    "Y1 = layer1.fit_transform(X)\n",
    "Y2 = layer2.fit_transform(Y1)\n",
    "Y3 = layer3.fit_transform(Y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "corexGroup1=pd.DataFrame(data=layer1.clusters, index=X.transpose().index)\n",
    "corexGroup1.columns=['corexGroup']\n",
    "\n",
    "mI=[]\n",
    "for ii in range(0, len(layer1.clusters)):\n",
    "    mI.append(layer1.mis[layer1.clusters[ii],ii])\n",
    "#len(mI)\n",
    "corexGroup1['mI']=mI\n",
    "\n",
    "corexGroup2=[]\n",
    "for ii in range(0, len(layer1.clusters)):\n",
    "    corexGroup2.append(layer2.clusters[layer1.clusters[ii]])\n",
    "corexGroup1['corexGroup2']=corexGroup2\n",
    "\n",
    "corexGroup1.to_csv('corexGroup1.csv')\n",
    "\n",
    "mutualInformation1=pd.DataFrame(data=mI, index=X.transpose().index)\n",
    "mutualInformation1.columns=['mI']\n",
    "mutualInformation1.to_csv('mutualInformation1.csv')\n",
    "\n",
    "totalCorrelation1=pd.DataFrame(data=layer1.tcs, index=list(range(len(layer1.tcs))))\n",
    "totalCorrelation1.columns=['totalCorrelation']\n",
    "totalCorrelation1.to_csv('totalCorrelation1.csv')\n",
    "\n",
    "labels1=pd.DataFrame(data=layer1.labels,index=X.index)\n",
    "labels1\n",
    "labels1.to_csv('labels1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mI2=[]\n",
    "for ii in range(0, len(layer2.clusters)):\n",
    "    mI2.append(layer2.mis[layer2.clusters[ii],ii])\n",
    "len(mI2)\n",
    "\n",
    "corexGroup2=pd.DataFrame(data=layer2.clusters,index=np.array(range(0, layer1.labels[0].size)))\n",
    "corexGroup2.columns=['corexGroup']\n",
    "corexGroup2['mI']=mI2\n",
    "corexGroup2.to_csv('corexGroup2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533, 20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.clusters # Each variable/column is associated with one Y_j\n",
    "#layer1.labels # Labels for each sample for Y_0\n",
    "#layer1.tcs # TC(X;Y_j) (all info measures reported in nats)\n",
    "layer1.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.999,  0.001,  0.   ],\n",
       "        [ 0.999,  0.001,  0.   ],\n",
       "        [ 0.   ,  0.   ,  1.   ],\n",
       "        ..., \n",
       "        [ 0.   ,  0.   ,  1.   ],\n",
       "        [ 1.   ,  0.   ,  0.   ],\n",
       "        [ 0.993,  0.   ,  0.007]],\n",
       "\n",
       "       [[ 0.137,  0.   ,  0.863],\n",
       "        [ 0.689,  0.   ,  0.311],\n",
       "        [ 0.   ,  0.007,  0.993],\n",
       "        ..., \n",
       "        [ 0.   ,  0.   ,  1.   ],\n",
       "        [ 0.989,  0.011,  0.   ],\n",
       "        [ 1.   ,  0.   ,  0.   ]],\n",
       "\n",
       "       [[ 0.   ,  0.   ,  1.   ],\n",
       "        [ 0.129,  0.871,  0.   ],\n",
       "        [ 0.   ,  1.   ,  0.   ],\n",
       "        ..., \n",
       "        [ 0.999,  0.   ,  0.001],\n",
       "        [ 0.028,  0.972,  0.   ],\n",
       "        [ 0.291,  0.   ,  0.709]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.001,  0.999,  0.   ],\n",
       "        [ 1.   ,  0.   ,  0.   ],\n",
       "        [ 0.   ,  0.005,  0.995],\n",
       "        ..., \n",
       "        [ 0.001,  0.879,  0.12 ],\n",
       "        [ 1.   ,  0.   ,  0.   ],\n",
       "        [ 0.   ,  0.067,  0.933]],\n",
       "\n",
       "       [[ 0.06 ,  0.011,  0.929],\n",
       "        [ 0.098,  0.   ,  0.902],\n",
       "        [ 0.004,  0.996,  0.001],\n",
       "        ..., \n",
       "        [ 0.216,  0.   ,  0.784],\n",
       "        [ 0.476,  0.524,  0.   ],\n",
       "        [ 0.011,  0.73 ,  0.26 ]],\n",
       "\n",
       "       [[ 0.999,  0.001,  0.   ],\n",
       "        [ 0.999,  0.001,  0.   ],\n",
       "        [ 0.   ,  0.   ,  1.   ],\n",
       "        ..., \n",
       "        [ 0.   ,  0.973,  0.027],\n",
       "        [ 0.912,  0.087,  0.001],\n",
       "        [ 0.071,  0.916,  0.013]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.p_y_given_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.   ],\n",
       "        [ 0.   ],\n",
       "        [ 0.   ],\n",
       "        ..., \n",
       "        [ 1.   ],\n",
       "        [ 0.   ],\n",
       "        [ 0.005]],\n",
       "\n",
       "       [[ 0.   ],\n",
       "        [ 0.   ],\n",
       "        [ 0.   ],\n",
       "        ..., \n",
       "        [ 0.014],\n",
       "        [ 0.   ],\n",
       "        [ 0.016]],\n",
       "\n",
       "       [[ 0.102],\n",
       "        [ 0.   ],\n",
       "        [ 0.006],\n",
       "        ..., \n",
       "        [ 0.005],\n",
       "        [ 0.048],\n",
       "        [ 0.009]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.   ],\n",
       "        [ 0.005],\n",
       "        [ 0.079],\n",
       "        ..., \n",
       "        [ 0.   ],\n",
       "        [ 0.16 ],\n",
       "        [ 0.   ]],\n",
       "\n",
       "       [[ 0.032],\n",
       "        [ 0.051],\n",
       "        [ 0.269],\n",
       "        ..., \n",
       "        [ 0.005],\n",
       "        [ 0.   ],\n",
       "        [ 0.   ]],\n",
       "\n",
       "       [[ 0.   ],\n",
       "        [ 0.15 ],\n",
       "        [ 0.026],\n",
       "        ..., \n",
       "        [ 0.   ],\n",
       "        [ 0.008],\n",
       "        [ 0.019]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01 , -0.   ,  0.01 , ...,  0.624,  0.   ,  0.   ],\n",
       "       [-0.   ,  0.035,  0.   , ..., -0.   ,  0.   , -0.   ],\n",
       "       [ 0.152, -0.   ,  0.012, ..., -0.   , -0.   , -0.   ],\n",
       "       ..., \n",
       "       [ 0.013, -0.   ,  0.062, ...,  0.   , -0.   ,  0.   ],\n",
       "       [ 0.033, -0.   ,  0.006, ...,  0.   ,  0.023, -0.   ],\n",
       "       [ 0.014,  0.026,  0.012, ..., -0.   ,  0.021, -0.   ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how much each gene is in each latent factor\n",
    "layer1.mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Python 3.6's help utility!\n",
      "\n",
      "If this is your first time using Python, you should definitely check out\n",
      "the tutorial on the Internet at http://docs.python.org/3.6/tutorial/.\n",
      "\n",
      "Enter the name of any module, keyword, or topic to get help on writing\n",
      "Python programs and using Python modules.  To quit this help utility and\n",
      "return to the interpreter, just type \"quit\".\n",
      "\n",
      "To get a list of available modules, keywords, symbols, or topics, type\n",
      "\"modules\", \"keywords\", \"symbols\", or \"topics\".  Each module also comes\n",
      "with a one-line summary of what it does; to list the modules whose name\n",
      "or summary contain a given string such as \"spam\", type \"modules spam\".\n",
      "\n",
      "\n",
      "You are now leaving help and returning to the Python interpreter.\n",
      "If you want to ask for help on a particular object directly from the\n",
      "interpreter, you can type \"help(object)\".  Executing \"help('string')\"\n",
      "has the same effect as typing a particular string at the help> prompt.\n"
     ]
    }
   ],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module corex:\n",
      "\n",
      "NAME\n",
      "    corex - Maximally Informative Representations using CORrelation EXplanation\n",
      "\n",
      "DESCRIPTION\n",
      "    Main ideas first described in:\n",
      "    Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
      "    Hierarchical Representations of High-Dimensional Data\"\n",
      "    AISTATS, 2015. arXiv preprint(arXiv:1410.7404.)\n",
      "    \n",
      "    The Bayesian smoothing option is described in:\n",
      "    Pepke and Ver Steeg, Comprehensive discovery of subsample gene expression components\n",
      "    by information explanation: therapeutic implications in cancer. BMC Medical Genomics, 2017.\n",
      "    \n",
      "    Code below written by: Greg Ver Steeg (gregv@isi.edu)\n",
      "    \n",
      "    License: Apache V2\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Corex\n",
      "    \n",
      "    class Corex(builtins.object)\n",
      "     |  Correlation Explanation\n",
      "     |  \n",
      "     |  A method to learn a hierarchy of successively more abstract\n",
      "     |  representations of complex data that are maximally\n",
      "     |  informative about the data. This method is unsupervised,\n",
      "     |  requires no assumptions about the data-generating model,\n",
      "     |  and scales linearly with the number of variables.\n",
      "     |  \n",
      "     |  Code follows sklearn naming/style (e.g. fit(X) to train)\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_hidden : int, optional, default=2\n",
      "     |      Number of hidden units.\n",
      "     |  \n",
      "     |  dim_hidden : int, optional, default=2\n",
      "     |      Each hidden unit can take dim_hidden discrete values.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      Maximum number of iterations before ending.\n",
      "     |  \n",
      "     |  n_repeat : int, optional\n",
      "     |      Repeat several times and take solution with highest TC.\n",
      "     |  \n",
      "     |  verbose : int, optional\n",
      "     |      The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
      "     |      2 output alpha matrix and MIs as you go.\n",
      "     |  \n",
      "     |  seed : integer or numpy.RandomState, optional\n",
      "     |      A random number generator instance to define the state of the\n",
      "     |      random permutations generator. If an integer is given, it fixes the\n",
      "     |      seed. Defaults to the global numpy random number generator.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  labels : array, [n_hidden, n_samples]\n",
      "     |      Label for each hidden unit for each sample.\n",
      "     |  \n",
      "     |  clusters : array, [n_visible]\n",
      "     |      Cluster label for each input variable.\n",
      "     |  \n",
      "     |  p_y_given_x : array, [n_hidden, n_samples, dim_hidden]\n",
      "     |      The distribution of latent factors for each sample.\n",
      "     |  \n",
      "     |  alpha : array-like, shape (n_components,)\n",
      "     |      Adjacency matrix between input variables and hidden units. In range [0,1].\n",
      "     |  \n",
      "     |  mis : array, [n_hidden, n_visible]\n",
      "     |      Mutual information between each (visible/observed) variable and hidden unit\n",
      "     |  \n",
      "     |  tcs : array, [n_hidden]\n",
      "     |      TC(X_Gj;Y_j) for each hidden unit\n",
      "     |  \n",
      "     |  tc : float\n",
      "     |      Convenience variable = Sum_j tcs[j]\n",
      "     |  \n",
      "     |  tc_history : array\n",
      "     |      Shows value of TC over the course of learning. Hopefully, it is converging.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
      "     |          High-Dimensional Data Through Correlation Explanation.\"\n",
      "     |          NIPS, 2014. arXiv preprint(arXiv:1406.1222.)\n",
      "     |  \n",
      "     |  [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
      "     |          Hierarchical Representations of High-Dimensional Data\"\n",
      "     |          AISTATS, 2015. arXiv preprint(arXiv:1410.7404.)\n",
      "     |  \n",
      "     |  [3]     Pepke and Ver Steeg, Comprehensive discovery of subsample\n",
      "     |          gene expression components by information explanation:\n",
      "     |          therapeutic implications in cancer. BMC Medical Genomics, 2017.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, n_hidden=2, dim_hidden=2, max_iter=100, n_repeat=1, ram=8.0, max_samples=10000, n_cpu=1, eps=1e-05, marginal_description='gaussian', smooth_marginals=False, missing_values=-1, seed=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  calculate_latent(self, theta, Xm)\n",
      "     |      \"Calculate the probability distribution for hidden factors for each sample.\n",
      "     |  \n",
      "     |  calculate_marginals_on_samples(self, theta, Xm, return_ratio=True)\n",
      "     |      Calculate the value of the marginal distribution for each variable, for each hidden variable and each sample.\n",
      "     |      \n",
      "     |      theta: array parametrizing the marginals\n",
      "     |      Xm: the data\n",
      "     |      returns log p(y_j|x_i)/p(y_j) for each j,sample,i,y_j. [n_hidden, n_samples, n_visible, dim_hidden]\n",
      "     |  \n",
      "     |  calculate_mis(self, p_y_given_x, theta, Xm)\n",
      "     |  \n",
      "     |  calculate_p_xi_given_y(self, xi, thetai)\n",
      "     |  \n",
      "     |  calculate_p_y(self, p_y_given_x)\n",
      "     |      Estimate log p(y_j) using a tiny bit of Laplace smoothing to avoid infinities.\n",
      "     |  \n",
      "     |  calculate_theta(self, Xm, p_y_given_x)\n",
      "     |      Estimate marginal parameters from data and expected latent labels.\n",
      "     |  \n",
      "     |  convergence(self)\n",
      "     |  \n",
      "     |  estimate_parameters(self, xi, p_y_given_x)\n",
      "     |  \n",
      "     |  estimate_se(self, xi, p_y_given_x, n_obs)\n",
      "     |  \n",
      "     |  estimate_sig(self, x_select, p_y_given_x, n_obs, prior)\n",
      "     |  \n",
      "     |  fit(self, X)\n",
      "     |      Fit CorEx on the data X. See fit_transform.\n",
      "     |  \n",
      "     |  fit_transform(self, X)\n",
      "     |      Fit CorEx on the data\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_visible]\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y: array-like, shape = [n_samples, n_hidden]\n",
      "     |         Learned values for each latent factor for each sample.\n",
      "     |         Y's are sorted so that Y_1 explains most correlation, etc.\n",
      "     |  \n",
      "     |  initialize_parameters(self, X)\n",
      "     |      Set up starting state\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_visible]\n",
      "     |          The data.\n",
      "     |  \n",
      "     |  initialize_representation(self)\n",
      "     |  \n",
      "     |  label(self, p_y_given_x)\n",
      "     |      Maximum likelihood labels for some distribution over y's\n",
      "     |  \n",
      "     |  load(self, filename)\n",
      "     |      Unpickle class instance. E.g., corex = ce.Marginal_Corex().load('saved.dat')\n",
      "     |  \n",
      "     |  marginal_p(self, xi, thetai)\n",
      "     |      Estimate marginals, log p(xi|yj) for each possible type.\n",
      "     |  \n",
      "     |  mi_bootstrap(self, Xm, n_permutation=20)\n",
      "     |  \n",
      "     |  normalize_latent(self, log_p_y_given_x_unnorm)\n",
      "     |      Normalize the latent variable distribution\n",
      "     |      \n",
      "     |      For each sample in the training set, we estimate a probability distribution\n",
      "     |      over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
      "     |      This normalization factor is quite useful as described in upcoming work.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Unnormalized distribution of hidden factors for each training sample.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p_y_given_x : 3D array, shape (n_hidden, n_samples, dim_hidden)\n",
      "     |          p(y_j|x^l), the probability distribution over all hidden factors,\n",
      "     |          for data samples l = 1...n_samples\n",
      "     |      log_z : 2D array, shape (n_hidden, n_samples)\n",
      "     |          Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
      "     |          used to estimate overall total correlation.\n",
      "     |  \n",
      "     |  print_verbose(self)\n",
      "     |  \n",
      "     |  save(self, filename)\n",
      "     |      Pickle a class instance. E.g., corex.save('saved.dat')\n",
      "     |  \n",
      "     |  sort_and_output(self, Xm)\n",
      "     |  \n",
      "     |  transform(self, X, details=False)\n",
      "     |      Label hidden factors for (possibly previously unseen) samples of data.\n",
      "     |      Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
      "     |      Returns: , shape = [n_samples, n_hidden]\n",
      "     |  \n",
      "     |  unique_info(self, correct)\n",
      "     |      *correct* has n_samples rows and n_hidden columns.\n",
      "     |      It indicates whether the ml estimate based on x_i for y_j is correct for sample l\n",
      "     |      Returns estimate of fraction of unique info in each predictor j=1...m\n",
      "     |  \n",
      "     |  update_alpha(self, p_y_given_x, theta, Xm, tcs)\n",
      "     |      A rule for non-tree CorEx structure.\n",
      "     |  \n",
      "     |  update_tc(self, log_z)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  clusters\n",
      "     |      Return cluster labels for variables\n",
      "     |  \n",
      "     |  labels\n",
      "     |      Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\n",
      "     |  \n",
      "     |  tc\n",
      "     |      The total correlation explained by all the Y's.\n",
      "\n",
      "FUNCTIONS\n",
      "    logsumexp2(z)\n",
      "        Multiprocessing pool.map requires a top-level function.\n",
      "    \n",
      "    unwrap_f(arg)\n",
      "        Multiprocessing pool.map requires a top-level function.\n",
      "\n",
      "DATA\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    /home/david/pythonCode/bio_corex/corex.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
